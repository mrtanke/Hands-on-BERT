{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b95f56-7920-4033-8a2d-d97eb30732a1",
   "metadata": {},
   "source": [
    "# 0. Initialie model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b88a141-f3cf-4d56-bd86-1259dbee5c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers.models.bert import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7a390b-dc4b-4de5-965d-3b7ba252202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92a34a7-144a-4d87-beb0-40c87fc70b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.55.4\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffc0fae2-27fa-4d5e-937b-99a83adf9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'this is a test sentence'\n",
    "\n",
    "model_input = tokenizer(test_sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5fa016-2d84-4908-a040-7a972252ed95",
   "metadata": {},
   "source": [
    "# 1. Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00991a6-b694-4de9-b6d5-14570c8d8ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1556, -0.0080, -0.0707,  ...,  0.0786,  0.0213,  0.0616],\n",
       "         [-0.5333,  0.5799,  0.1044,  ...,  0.0241,  0.4888,  0.0161],\n",
       "         [-1.0609, -0.3058, -0.5043,  ...,  0.1874,  0.2874,  0.4032],\n",
       "         ...,\n",
       "         [ 0.8206, -0.6656, -0.7054,  ...,  0.1347,  0.1117, -1.9040],\n",
       "         [ 1.1128,  0.6603, -0.1509,  ...,  0.3253, -1.0006, -1.9106],\n",
       "         [-0.0736,  0.0346,  0.0376,  ..., -0.4506,  0.6585, -0.0502]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(**model_input)\n",
    "\n",
    "# Output of first layer\n",
    "output[2][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f47384f-f3be-4da4-a331-db0478d0d604",
   "metadata": {},
   "source": [
    "# 2. from scratch\n",
    "\n",
    "- BertLayer\n",
    "    - attention: BertAttention\n",
    "        - self: BertSelfAttention\n",
    "        - output: BertSelfOutput\n",
    "    - Feed Forward\n",
    "        - intermediate: BertIntermediate, 768=>4*768\n",
    "        - output: BertOutput, 4*768=>768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86af96b-ff70-41be-973b-0db399826af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLayer(\n",
       "  (attention): BertAttention(\n",
       "    (self): BertSdpaSelfAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (output): BertSelfOutput(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (intermediate): BertIntermediate(\n",
       "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (intermediate_act_fn): GELUActivation()\n",
       "  )\n",
       "  (output): BertOutput(\n",
       "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = output[2][0]\n",
    "\n",
    "layer = model.encoder.layer[0]\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee21d10-e139-464a-bdc9-e949b8b4ceac",
   "metadata": {},
   "source": [
    "## 2.1 First add & norm, in mha\n",
    "mha -> multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f9e55b7-9da0-4404-b7cb-a8e11d12f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Multi-Head Attention\n",
    "mha_output = layer.attention.self(embeddings)\n",
    "\n",
    "# Output of Add & Norm\n",
    "attn_output = layer.attention.output(mha_output[0], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27220a-6452-4895-a4fe-912bd4084df4",
   "metadata": {},
   "source": [
    "## 2.2 First add & norm, in mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44b348ff-5d52-4861-9cb6-c3d0e40e17a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 3072])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output of Feed Forward\n",
    "mlp1 = layer.intermediate(attn_output)\n",
    "\n",
    "mlp1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "009e6a46-458c-42b7-b124-363911a2ab02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1556, -0.0080, -0.0707,  ...,  0.0786,  0.0213,  0.0616],\n",
       "         [-0.5333,  0.5799,  0.1044,  ...,  0.0241,  0.4888,  0.0161],\n",
       "         [-1.0609, -0.3058, -0.5043,  ...,  0.1874,  0.2874,  0.4032],\n",
       "         ...,\n",
       "         [ 0.8206, -0.6656, -0.7054,  ...,  0.1347,  0.1117, -1.9040],\n",
       "         [ 1.1128,  0.6603, -0.1509,  ...,  0.3253, -1.0006, -1.9106],\n",
       "         [-0.0736,  0.0346,  0.0376,  ..., -0.4506,  0.6585, -0.0502]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output of Addd & Norm\n",
    "mlp2 = layer.output(mlp1, attn_output)\n",
    "\n",
    "mlp2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (bert)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
