{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afde18a9-dcd6-4c45-a8a6-ca4b87acc4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'albums sold 124443286539 copies'\n",
    "s2 = 'technically perfect, melodically correct'\n",
    "s3 = 'featuring a previously unheard track'\n",
    "s4 = 'bestselling music artist'\n",
    "s5 = 's1 d1 o1 and o2'\n",
    "s6 = 'asbofwheohwbeif'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4582a672-0c99-480c-b22f-8b1535f1f3c6",
   "metadata": {},
   "source": [
    "# 0. Instantiate the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6a3d6b-5d0e-43dc-8d26-00ed0a7fce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f2df5-59be-456e-9ff6-89e8abac444a",
   "metadata": {},
   "source": [
    "# 1. vocab\n",
    "- tokenizer.vacab vs tokenizer.ids_to_tokens\n",
    "- len(tokenizer.vocab) == 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cbff5d5-4aa6-4b4f-b8fa-ae8d8853ee7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30522, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.vocab), tokenizer.vocab['[UNK]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdbd1a8-b2a7-4dd0-b835-c3aec6147cd7",
   "metadata": {},
   "source": [
    "# 2. Test by sample subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83697c08-3a67-4aae-89f5-a99d1f52d6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 4042, 2853, 13412, 22932, 16703, 20842, 22275, 2683, 4809, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] albums sold 124443286539 copies [SEP]\n",
      "['[CLS]', 'albums', 'sold', '124', '##44', '##32', '##86', '##53', '##9', 'copies', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(s1)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83fe1a52-8dfb-4f4e-953d-4d6fc17a902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 10892, 3819, 1010, 17187, 3973, 6149, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] technically perfect, melodically correct [SEP]\n",
      "['[CLS]', 'technically', 'perfect', ',', 'melodic', '##ally', 'correct', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(s2)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a9077bd-23ea-45ae-9eaf-047270a0ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 3794, 1037, 3130, 4895, 26362, 2650, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] featuring a previously unheard track [SEP]\n",
      "['[CLS]', 'featuring', 'a', 'previously', 'un', '##heard', 'track', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(s3)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84769f56-d164-477f-8a4b-ed781ae28dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2190, 23836, 2075, 2189, 3063, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] bestselling music artist [SEP]\n",
      "['[CLS]', 'best', '##sell', '##ing', 'music', 'artist', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(s4)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cfa545c-a41e-4178-83e4-39e6acbfe003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1055, 2487, 1040, 2487, 1051, 2487, 1998, 1051, 2475, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] s1 d1 o1 and o2 [SEP]\n",
      "['[CLS]', 's', '##1', 'd', '##1', 'o', '##1', 'and', 'o', '##2', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(s5)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad738940-7598-46cd-90b6-19c685c26745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2004, 5092, 2546, 2860, 5369, 11631, 2860, 19205, 2546, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[CLS] asbofwheohwbeif [SEP]\n",
      "['[CLS]', 'as', '##bo', '##f', '##w', '##he', '##oh', '##w', '##bei', '##f', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(s6)\n",
    "print(inputs)\n",
    "print(tokenizer.decode(inputs['input_ids']))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71b88-27a6-401a-817b-9fd5536f87c8",
   "metadata": {},
   "source": [
    "# 3. Summary\n",
    "- tokenizer will not easily convert a word into [UNK] (100)\n",
    "- based on vocab table -> tokenize, encode, decode in one\n",
    "    - tokenize：word -> token(s)，Map the word into keys in vocab as accurately as possible\n",
    "    - encode: token -> id\n",
    "    - decode: id -> token -> word\n",
    "    - encoding isn't the end, decoding should also restore the IDs into words that closely match the original input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d734bad-9cad-4c2e-97b7-b91579270bf6",
   "metadata": {},
   "source": [
    "The main aim of a vocabulary (vocab) in NLP models is exactly to map words (or subwords/tokens) into integer IDs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (bert)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
